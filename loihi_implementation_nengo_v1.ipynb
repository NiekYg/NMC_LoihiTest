{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adead06-1b88-421f-9dec-0e0d50769b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nengo in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: nengo-loihi in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (1.1.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.13 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from nengo) (1.24.4)\n",
      "Requirement already satisfied: packaging in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from nengo-loihi) (23.2)\n",
      "Requirement already satisfied: scipy>=1.2.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from nengo-loihi) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from nengo-loihi) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from jinja2->nengo-loihi) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nengo nengo-loihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be32a59-e093-4f3b-b08f-ab6837de5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "import nengo_loihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c20de2-ce4d-4c5d-adb3-17e6e38370f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "val_path = 'final_project/val_dataset.pth'\n",
    "train_path = 'final_project/train_dataset.pth'\n",
    "# Load the model\n",
    "test_val_dataset_loaded= torch.load(val_path)\n",
    "train_dataset_loaded= torch.load(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f41a5bac-f738-422d-8f24-767a5ed6b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_npz_events(file_path):\n",
    "    \"\"\"\n",
    "    Load and return the content of an .npz file containing event data.\n",
    "    :param file_path: The path to the .npz file.\n",
    "    :return: A dictionary with keys 't', 'x', 'y', 'p'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Assuming the file format is not zip\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        return {key: data[key] for key in ['t', 'x', 'y', 'p']}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b46bf95-0a3f-47bf-bc46-a4087ae6c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "def split_to_train_test_set(train_ratio: float, origin_dataset: torch.utils.data.Dataset, num_classes: int, random_split: bool = False):\n",
    "    '''\n",
    "    :param train_ratio: split the ratio of the origin dataset as the train set\n",
    "    :type train_ratio: float\n",
    "    :param origin_dataset: the origin dataset\n",
    "    :type origin_dataset: torch.utils.data.Dataset\n",
    "    :param num_classes: total classes number, e.g., ``10`` for the MNIST dataset\n",
    "    :type num_classes: int\n",
    "    :param random_split: If ``False``, the front ratio of samples in each classes will\n",
    "            be included in train set, while the reset will be included in test set.\n",
    "            If ``True``, this function will split samples in each classes randomly. The randomness is controlled by\n",
    "            ``numpy.random.seed``\n",
    "    :type random_split: int\n",
    "    :return: a tuple ``(train_set, test_set)``\n",
    "    :rtype: tuple\n",
    "    '''\n",
    "    label_idx = []\n",
    "    for i in range(num_classes):\n",
    "        label_idx.append([])\n",
    "\n",
    "    for i, item in enumerate(tqdm.tqdm(origin_dataset)):\n",
    "\n",
    "        y = item[1]\n",
    "\n",
    "        if isinstance(y, np.ndarray) or isinstance(y, torch.Tensor):\n",
    "            y = y.item()\n",
    "        label_idx[y].append(i)\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    if random_split:\n",
    "        for i in range(num_classes):\n",
    "            np.random.shuffle(label_idx[i])\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        pos = math.ceil(label_idx[i].__len__() * train_ratio)\n",
    "        train_idx.extend(label_idx[i][0: pos])\n",
    "        test_idx.extend(label_idx[i][pos: label_idx[i].__len__()])\n",
    "\n",
    "    return torch.utils.data.Subset(origin_dataset, train_idx), torch.utils.data.Subset(origin_dataset, test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a49d41-3f97-4ba2-a02b-363197ea0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN to USE THE WHOLE DATASET\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CustomNPZDataset(Dataset):\n",
    "    def __init__(self, root_dir, extensions=('.npz',)):\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_folder = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_folder):\n",
    "                for file in os.listdir(cls_folder):\n",
    "                    if file.endswith(extensions):\n",
    "                        self.file_paths.append(os.path.join(cls_folder, file))\n",
    "                        self.labels.append(self.class_to_idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        data = load_npz_events(file_path)  # Your custom loader function\n",
    "        label = self.labels[idx]\n",
    "        return data, label\n",
    "\n",
    "# Replace 'root_dir' with your dataset directory\n",
    "dataset = CustomNPZDataset(root_dir='extracted_content/content/CIFAR/events_np')\n",
    "\n",
    "# Splitting the dataset\n",
    "train_size = int(0.7 * len(dataset))  # 70% of data for training\n",
    "test_size = len(dataset) - train_size  # Remaining 30% for testing/validation\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# Optionally create DataLoader instances for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c185c8e-52d4-40e3-9949-71e5c8923a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully.\n",
      "Keys in the file: ['t', 'x', 'y', 'p']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_file_path = 'extracted_content/content/CIFAR/events_np/airplane/cifar10_airplane_975.npz'  # Replace with the actual file path\n",
    "try:\n",
    "    data = np.load(test_file_path)\n",
    "    print(\"File loaded successfully.\")\n",
    "    print(\"Keys in the file:\", list(data.keys()))\n",
    "except Exception as e:\n",
    "    print(\"Error loading the file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "316be2af-d9a0-4852-afcf-c3ee0a743cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load file: extracted_content/content/CIFAR/events_np/automobile/cifar10_automobile_207.npz\n",
      "Error loading the file extracted_content/content/CIFAR/events_np/automobile/cifar10_automobile_207.npz: File is not a zip file\n",
      "(None, 0)\n"
     ]
    }
   ],
   "source": [
    "#run to USE 5 images per category \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class CustomNPZDataset(Dataset):\n",
    "    def __init__(self, root_dir, extensions=('.npz',), samples_per_class=5): #USE 5 IMAGES PER CLASS\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_folder = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_folder):\n",
    "                cls_files = [file for file in os.listdir(cls_folder) if file.endswith(extensions)]\n",
    "                cls_files = cls_files[:samples_per_class]  # Select only a certain number of samples\n",
    "                for file in cls_files:\n",
    "                    self.file_paths.append(os.path.join(cls_folder, file))\n",
    "                    self.labels.append(self.class_to_idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        data = load_npz_events(file_path)\n",
    "        if data is None:\n",
    "            # Handle the case where data couldn't be loaded\n",
    "            # For example, you can skip this item, return an empty data, etc.\n",
    "            pass\n",
    "        label = self.labels[idx]\n",
    "        return data, label\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomNPZDataset(root_dir='extracted_content/content/CIFAR/events_np')\n",
    "print(dataset[0])\n",
    "\n",
    "# Splitting the dataset\n",
    "train_size = int(0.7 * len(dataset))  # Adjust the ratio as needed\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "# create DataLoader instances for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "817cdfca-c737-43a5-9151-11e527fc6d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Structure:\n",
      "t: shape = (148123,), sample = [ 0  2  7 11 16 21 25 29 34 38]\n",
      "x: shape = (148123,), sample = [117 113 112 115 114 124 125 127 126 120]\n",
      "y: shape = (148123,), sample = [60 60 60 60 60 60 60 60 60 60]\n",
      "p: shape = (148123,), sample = [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def print_original_data_structure(event_data):\n",
    "    print(\"Original Data Structure:\")\n",
    "    for key in event_data:\n",
    "        print(f\"{key}: shape = {event_data[key].shape}, sample = {event_data[key][:10]}\")\n",
    "\n",
    "\n",
    "event_data = np.load('extracted_content/content/CIFAR/events_np/airplane/cifar10_airplane_979.npz')\n",
    "print_original_data_structure(event_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9aef67c-7f64-46b5-aed2-f0249ad88bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, batch_size=32, split_ratios=(0.7, 0.15, 0.15), bin_size=10000):\n",
    "    \"\"\"\n",
    "    Load and preprocess CIFAR-10 DVS data, then split it into train, validation, and test sets.\n",
    "    :param dataset: The CustomNPZDataset instance.\n",
    "    :param batch_size: Batch size for DataLoader.\n",
    "    :param split_ratios: Tuple of ratios for splitting dataset (train, validation, test).\n",
    "    :param bin_size: Size of each time bin in microseconds for temporal binning.\n",
    "    :return: DataLoaders for train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Calculate sizes for train, validation, and test sets\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(split_ratios[0] * total_size)\n",
    "    val_size = int(split_ratios[1] * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Randomly split dataset\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # DataLoader for each set\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to apply preprocessing steps to each batch.\n",
    "    \"\"\"\n",
    "    processed_batch = []\n",
    "    labels = []\n",
    "    for data, label in batch:\n",
    "        # Apply preprocessing functions\n",
    "        data = preprocess_cifar10_dvs_data(data)\n",
    "        data = temporal_binning(data, bin_size=10000)  # Adjust bin size as needed\n",
    "        data = [normalize_data(frame, method='min-max') for frame in data]\n",
    "        processed_batch.append(data)\n",
    "        labels.append(label)\n",
    "    return torch.stack(processed_batch), torch.tensor(labels)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomNPZDataset(root_dir='extracted_content/content/CIFAR/events_np')\n",
    "\n",
    "# Get DataLoaders\n",
    "train_loader, val_loader, test_loader = get_data(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430f2ce6-1ba4-43e4-b059-1ae996c3df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cifar10_dvs_data(event_data, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Preprocess CIFAR-10 DVS event data into a format suitable for the SNN.\n",
    "    :param event_data: CIFAR-10 DVS data (npz file contents).\n",
    "    :param img_size: Tuple of the image size (height, width).\n",
    "    :return: Preprocessed data.\n",
    "    \"\"\"\n",
    "    height, width = img_size\n",
    "    processed_data = np.zeros((height, width, 2))  # Assuming 2 channels for polarity\n",
    "\n",
    "    for t, x, y, p in zip(event_data['t'], event_data['x'], event_data['y'], event_data['p']):\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            processed_data[y, x, int(p)] = 1  # Set spike at (x, y) for the given polarity\n",
    "\n",
    "    return processed_data.flatten()  # Flatten the array to match the input layer dimensions\n",
    "\n",
    "def temporal_binning(event_data, bin_size, img_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Temporally bin the CIFAR-10 DVS event data.\n",
    "    :param event_data: CIFAR-10 DVS data (npz file contents).\n",
    "    :param bin_size: Size of each time bin in microseconds.\n",
    "    :param img_size: Tuple of the image size (height, width).\n",
    "    :return: List of binned data.\n",
    "    \"\"\"\n",
    "    max_time = np.max(event_data['t'])\n",
    "    num_bins = int(np.ceil(max_time / bin_size))\n",
    "    height, width = img_size\n",
    "    binned_data = [np.zeros((height, width, 2)) for _ in range(num_bins)]\n",
    "\n",
    "    for t, x, y, p in zip(event_data['t'], event_data['x'], event_data['y'], event_data['p']):\n",
    "        if 0 <= x < width and 0 <= y < height:\n",
    "            bin_index = int(t // bin_size)\n",
    "            binned_data[bin_index][y, x, int(p)] = 1\n",
    "\n",
    "    return [bin_data.flatten() for bin_data in binned_data]\n",
    "\n",
    "def normalize_data(data, method='min-max'):\n",
    "    \"\"\"\n",
    "    Normalize the input data.\n",
    "    :param data: The data to be normalized (expected as a flattened array).\n",
    "    :param method: The method of normalization ('min-max' or 'standard').\n",
    "    :return: Normalized data.\n",
    "    \"\"\"\n",
    "    if method == 'min-max':\n",
    "        data_min = np.min(data)\n",
    "        data_max = np.max(data)\n",
    "        return (data - data_min) / (data_max - data_min)\n",
    "    elif method == 'standard':\n",
    "        data_mean = np.mean(data)\n",
    "        data_std = np.std(data)\n",
    "        return (data - data_mean) / data_std\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bff8ff-2069-49b6-8969-fbbf76a242af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Structure:\n",
      "Shape = (32768,), sample = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Processed Data Structure:\n",
      "Number of bins: 135\n",
      "Bin 0: shape = (32768,), sample = [1. 0. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
      "Bin 1: shape = (32768,), sample = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Bin 2: shape = (32768,), sample = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Processed Data Structure:\n",
      "Shape = (32768,), sample = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def print_processed_data_structure(processed_data):\n",
    "    print(\"Processed Data Structure:\")\n",
    "    if isinstance(processed_data, list):  # For temporally binned data\n",
    "        print(f\"Number of bins: {len(processed_data)}\")\n",
    "        for i, bin_data in enumerate(processed_data[:3]):  # Print first few bins\n",
    "            print(f\"Bin {i}: shape = {bin_data.shape}, sample = {bin_data[:10]}\")\n",
    "    else:  # For flattened data\n",
    "        print(f\"Shape = {processed_data.shape}, sample = {processed_data[:10]}\")\n",
    "\n",
    "#Apply preprocessing and then print structure\n",
    "processed_data = preprocess_cifar10_dvs_data(event_data)\n",
    "print_processed_data_structure(processed_data)\n",
    "\n",
    "#For temporal binning\n",
    "binned_data = temporal_binning(event_data, bin_size=10000)\n",
    "print_processed_data_structure(binned_data)\n",
    "\n",
    "# For normalization\n",
    "normalized_data = normalize_data(processed_data, method='min-max')\n",
    "print_processed_data_structure(normalized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7accd32-c74c-405b-958f-d0de3385626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "from nengo_loihi import Simulator\n",
    "\n",
    "# Define the network structure\n",
    "with nengo.Network() as net:\n",
    "    # Loihi compatible LIF neuron parameters\n",
    "    lif_params = {\n",
    "        'tau_rc': 0.02,  # membrane time constant\n",
    "        'tau_ref': 0.002, # refractory period\n",
    "        # other parameters as needed\n",
    "    }\n",
    "\n",
    "    # Input layer - adapt size as needed\n",
    "    input_layer = nengo.Ensemble(n_neurons=128*128*2, dimensions=1, neuron_type=nengo.LIF(**lif_params))\n",
    "\n",
    "    # Hidden layers\n",
    "    hidden_layer_1 = nengo.Ensemble(512, 1, neuron_type=nengo.LIF(**lif_params))\n",
    "    hidden_layer_2 = nengo.Ensemble(512, 1, neuron_type=nengo.LIF(**lif_params))\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = nengo.Ensemble(10, 1, neuron_type=nengo.LIF(**lif_params))\n",
    "\n",
    "    # Connections\n",
    "    nengo.Connection(input_layer, hidden_layer_1)\n",
    "    nengo.Connection(hidden_layer_1, hidden_layer_2)\n",
    "    nengo.Connection(hidden_layer_2, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e7a9e9b-e4f8-46ec-8267-7b39cf52f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the file extracted_content/content/CIFAR/events_np/automobile/cifar10_automobile_207.npz: File is not a zip file\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomNPZDataset(root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_content/content/CIFAR/events_np\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Use the modified get_data() with the dataset instance\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m original_train_data, original_val_data, original_test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m, in \u001b[0;36mpreprocess_and_split_data\u001b[0;34m(dataset, bin_size, normalization_method, split_ratios)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[1;32m      8\u001b[0m     data, label \u001b[38;5;241m=\u001b[39m dataset[i]\n\u001b[0;32m----> 9\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_cifar10_dvs_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     binned \u001b[38;5;241m=\u001b[39m temporal_binning(processed, bin_size\u001b[38;5;241m=\u001b[39mbin_size)\n\u001b[1;32m     11\u001b[0m     normalized \u001b[38;5;241m=\u001b[39m [normalize_data(frame\u001b[38;5;241m.\u001b[39mflatten(), method\u001b[38;5;241m=\u001b[39mnormalization_method) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m binned]\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mpreprocess_cifar10_dvs_data\u001b[0;34m(event_data, img_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m height, width \u001b[38;5;241m=\u001b[39m img_size\n\u001b[1;32m      9\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((height, width, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Assuming 2 channels for polarity\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, x, y, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mevent_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, event_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], event_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m], event_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m<\u001b[39m width \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m<\u001b[39m height:\n\u001b[1;32m     13\u001b[0m         processed_data[y, x, \u001b[38;5;28mint\u001b[39m(p)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Set spike at (x, y) for the given polarity\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def preprocess_and_split_data(dataset, bin_size=10000, normalization_method='min-max', split_ratios=(0.7, 0.15, 0.15)):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into training, validation, and testing sets.\n",
    "    \"\"\"\n",
    "    # Preprocess all data\n",
    "    preprocessed_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        data, label = dataset[i]\n",
    "        processed = preprocess_cifar10_dvs_data(data)\n",
    "        binned = temporal_binning(processed, bin_size=bin_size)\n",
    "        normalized = [normalize_data(frame.flatten(), method=normalization_method) for frame in binned]\n",
    "        preprocessed_data.append((normalized, label))\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_size = len(preprocessed_data)\n",
    "    train_size = int(split_ratios[0] * total_size)\n",
    "    val_size = int(split_ratios[1] * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Split the data\n",
    "    train_data = preprocessed_data[:train_size]\n",
    "    val_data = preprocessed_data[train_size:train_size + val_size]\n",
    "    test_data = preprocessed_data[train_size + val_size:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomNPZDataset(root_dir='extracted_content/content/CIFAR/events_np')\n",
    "\n",
    "# Use the modified get_data() with the dataset instance\n",
    "original_train_data, original_val_data, original_test_data = preprocess_and_split_data(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888c54f-ce7a-4d1b-a26b-7d875f885932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulator configuration for Loihi\n",
    "with Simulator(net, precompute=True) as sim:\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_data:\n",
    "            # Run the network with the current inputs\n",
    "            for input_frame in inputs:\n",
    "                sim.run(input_frame)\n",
    "            # Apply learning and error computation here\n",
    "\n",
    "    # Validation\n",
    "    for inputs, targets in val_data:\n",
    "        for input_frame in inputs:\n",
    "            sim.run(input_frame)\n",
    "        # Evaluate the network's performance on the validation set\n",
    "\n",
    "    # Testing\n",
    "    for inputs, targets in test_data:\n",
    "        for input_frame in inputs:\n",
    "            sim.run(input_frame)\n",
    "        # Evaluate the network's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c2712-e0f3-4b2a-97c8-9736dc1bd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulator configuration for Loihi\n",
    "with Simulator(net, precompute=True) as sim:\n",
    "    train_data, val_data, test_data = get_data()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_data:\n",
    "            # Preprocess each input\n",
    "            inputs = preprocess_cifar10_dvs_data(inputs)\n",
    "            inputs = temporal_binning(inputs, bin_size=100)  # Example bin size\n",
    "            inputs = normalize_data(inputs.flatten(), method='min-max')\n",
    "            sim.run(inputs)\n",
    "\n",
    "            # Define learning connections with a learning rule\n",
    "            learning_conn = nengo.Connection(hidden_layer_1, hidden_layer_2, \n",
    "                                             learning_rule_type=nengo.PES(learning_rate=1e-4))\n",
    "\n",
    "            # Error population for learning\n",
    "            error = nengo.Ensemble(n_neurons=10, dimensions=1)\n",
    "            nengo.Connection(output_layer, error)\n",
    "            nengo.Connection(error, learning_conn.learning_rule)\n",
    "\n",
    "\n",
    "     # Validation\n",
    "    for inputs, targets in val_data:\n",
    "        # Preprocess inputs (already done in the DataLoader)\n",
    "        sim.run(inputs)\n",
    "        # Evaluate the network's performance on the validation set\n",
    "        # Code for evaluation goes here\n",
    "\n",
    "    # Testing\n",
    "    for inputs, targets in test_data:\n",
    "        # Preprocess inputs (already done in the DataLoader)\n",
    "        sim.run(inputs)\n",
    "        # Evaluate the network's performance on the test set\n",
    "        # Code for evaluation goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a368440-b025-4d22-b629-e972a5b0ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nengo\n",
    "from nengo_loihi import Simulator\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "train_data, val_data, test_data = load_data()  # You need to define this function\n",
    "preprocessed_train_data = preprocess_data(train_data)\n",
    "preprocessed_val_data = preprocess_data(val_data)\n",
    "\n",
    "\n",
    "\n",
    "# Simulator configuration for Loihi\n",
    "with Simulator(net, precompute=True) as sim:\n",
    "    # Training and validation loop\n",
    "    for epoch in range(1, episodes + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training loop\n",
    "        for input, label in preprocessed_train_data:\n",
    "            sim.run(input)\n",
    "            # Implement learning here (if applicable)\n",
    "\n",
    "        # Validation loop\n",
    "        latency_sum = 0\n",
    "        num_batches = 0\n",
    "        throughput_start_time = time.time()\n",
    "        for input, label in preprocessed_val_data:\n",
    "            batch_start_time = time.time()\n",
    "            sim.run(input)\n",
    "            batch_end_time = time.time()\n",
    "            latency_batch = batch_end_time - batch_start_time\n",
    "            latency_sum += latency_batch\n",
    "            num_batches += 1\n",
    "\n",
    "        throughput_end_time = time.time()\n",
    "        average_latency = latency_sum / num_batches if num_batches > 0 else 0\n",
    "        throughput_time = throughput_end_time - throughput_start_time\n",
    "        throughput = num_batches / throughput_time if throughput_time > 0 else 0\n",
    "\n",
    "        epoch_latency.append(average_latency)\n",
    "        epoch_throughputs.append(throughput)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Epoch {epoch} completed in {elapsed_time} seconds')\n",
    "\n",
    "# Final calculations and output\n",
    "print(f\"Average Latency over all epochs: {np.mean(epoch_latency)}\")\n",
    "print(f\"Average Throughput over all epochs: {np.mean(epoch_throughputs)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
