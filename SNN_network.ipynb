{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7699523b-9023-4406-8add-f2f8fced83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d04dbd-7194-4d8e-8b48-8392452331ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (0.16.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: triton==2.1.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: jinja2 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: requests in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d512f2-b3a4-4bf2-b76c-909beb9f8f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spikingjelly in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (0.0.0.0.14)\n",
      "Requirement already satisfied: torch in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from spikingjelly) (2.1.1)\n",
      "Requirement already satisfied: torchvision in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from spikingjelly) (0.16.1)\n",
      "Requirement already satisfied: numpy in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from spikingjelly) (1.24.3)\n",
      "Requirement already satisfied: scipy in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from spikingjelly) (1.10.1)\n",
      "Requirement already satisfied: tqdm in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from spikingjelly) (4.66.1)\n",
      "Requirement already satisfied: matplotlib in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from spikingjelly) (3.7.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (1.1.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (10.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (4.43.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (23.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (6.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from matplotlib->spikingjelly) (3.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->spikingjelly) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
      "Requirement already satisfied: jinja2 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (2023.10.0)\n",
      "Requirement already satisfied: filelock in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (4.8.0)\n",
      "Requirement already satisfied: networkx in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torch->spikingjelly) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->spikingjelly) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from jinja2->torch->spikingjelly) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from sympy->torch->spikingjelly) (1.3.0)\n",
      "Requirement already satisfied: requests in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from torchvision->spikingjelly) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision->spikingjelly) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision->spikingjelly) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision->spikingjelly) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages (from requests->torchvision->spikingjelly) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install spikingjelly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba0bc74-66d9-4665-bfc0-0a9c76a13087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_dataset = torch.load(\"final_project/train_dataset.pth\")\n",
    "val_dataset = torch.load(\"final_project/val_dataset.pth\")\n",
    "\n",
    "# Explore the datasets\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53da99bf-9c1d-471f-b2a6-d55dfee9a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import spikingjelly.clock_driven.encoding as encoding\n",
    "\n",
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, tau_syn=5.0, v_threshold=1.0, v_reset=0.0, initial_v=-1.0):\n",
    "        super(LIFNeuron, self).__init__()\n",
    "        self.tau_syn = tau_syn\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.initial_v = initial_v\n",
    "        self.synaptic_current = torch.zeros(1)\n",
    "\n",
    "    def forward(self, synaptic_input):\n",
    "        self.synaptic_current = torch.exp(-1.0 / self.tau_syn) * self.synaptic_current + synaptic_input\n",
    "        spike = (self.synaptic_current >= self.v_threshold).float()\n",
    "        self.synaptic_current = torch.where(spike > 0, torch.tensor(self.v_reset), self.synaptic_current)\n",
    "        return spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9596cbee-cbc6-4998-bd64-7d569ae7c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN(\n",
      "  (neuron): LIFNeuron()\n",
      "  (input_encoding): PoissonEncoder()\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SNN, self).__init__()\n",
    "\n",
    "        # Define LIF neurons and encoding\n",
    "        self.neuron = LIFNeuron()\n",
    "        self.input_encoding = encoding.PoissonEncoder()\n",
    "\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input encoding\n",
    "        x = self.input_encoding(x)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "\n",
    "        # LIF Neuron layer\n",
    "        x = self.neuron(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize the SNN\n",
    "input_size = (3, 32, 32)  # Assuming CIFAR-10 image dimensions\n",
    "num_classes = 10\n",
    "snn = SNN(input_size, num_classes)\n",
    "\n",
    "# Print the SNN architecture\n",
    "print(snn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45debb47-d69d-489a-9ef3-b0c7780d6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193d840a-780e-4f3c-bdde-48cbf8604ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c032e1c-3657-41b8-ae5a-35fbfdcfead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(snn.parameters(), lr=0.001)  # Adjust the learning rate as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f9a858-a59f-404a-9bf7-723deb5a8bc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     snn\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m snn(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    snn.train()  # Set the model to training mode\n",
    "    for (inputs, labels) in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = snn(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    # Validation\n",
    "    snn.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = snn(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8545dbe8-26ae-4404-820c-a35152853d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/CIFAR/frames_number_10_split_by_number/automobile/cifar10_automobile_298.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have loaded your datasets into train_loader and val_loader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example for training data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data shape - Inputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example for validation data\u001b[39;00m\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/spikingjelly/datasets/__init__.py:169\u001b[0m, in \u001b[0;36mload_npz_frames\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_npz_frames\u001b[39m(file_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    :param file_name: path of the npz file that saves the frames\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    :type file_name: str\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    :return: frames\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    :rtype: np.ndarray\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/nengoloihi/miniconda/envs/nengoloihi38/lib/python3.8/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/CIFAR/frames_number_10_split_by_number/automobile/cifar10_automobile_298.npz'"
     ]
    }
   ],
   "source": [
    "# Assuming we have loaded your datasets into train_loader and val_loader\n",
    "\n",
    "# Example for training data\n",
    "inputs, labels = next(iter(train_loader))\n",
    "print(\"Training data shape - Inputs:\", inputs.shape, \"Labels:\", labels.shape)\n",
    "\n",
    "# Example for validation data\n",
    "val_inputs, val_labels = next(iter(val_loader))\n",
    "print(\"Validation data shape - Inputs:\", val_inputs.shape, \"Labels:\", val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be947f57-25dc-4b11-a0d5-608e59ac1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "content/CIFAR/frames_number_10_split_by_number/airplane/cifar10_airplane_0.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "117e9189-aae1-49c4-b447-f2c5e03563e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_dataset = \"train\"\n",
    "val_dataset = \"validation\"\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fd2ee-321e-46e4-8c8c-4b4e5f63a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_dataset = torch.load(\"final_project/train_dataset.pth\")\n",
    "val_dataset = torch.load(\"final_project/val_dataset.pth\")\n",
    "\n",
    "# Print the content of the datasets\n",
    "print(\"Train Dataset:\", train_dataset)\n",
    "print(\"Val Dataset:\", val_dataset)\n",
    "\n",
    "# Access the indices of the subsets\n",
    "train_indices = train_dataset.indices\n",
    "val_indices = val_dataset.indices\n",
    "\n",
    "# Print a few example indices\n",
    "print(\"Train Indices:\", train_indices[:5])\n",
    "print(\"Val Indices:\", val_indices[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c740b11b-7444-4bed-af5b-bd1090b194db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Path to the root directory containing the 10 category folders\n",
    "root_directory = \"frames_number_10_split_by_number\"\n",
    "\n",
    "# Create directories for the training and validation sets\n",
    "train_directory = \"train\"\n",
    "val_directory = \"validation\"\n",
    "os.makedirs(train_directory, exist_ok=True)\n",
    "os.makedirs(val_directory, exist_ok=True)\n",
    "\n",
    "# Percentage of data to be used for validation\n",
    "validation_percentage = 0.3\n",
    "\n",
    "# Iterate through each category folder\n",
    "for category_folder in os.listdir(root_directory):\n",
    "    category_path = os.path.join(root_directory, category_folder)\n",
    "\n",
    "    # Get the list of files in the category folder\n",
    "    files = os.listdir(category_path)\n",
    "\n",
    "    # Calculate the number of files to be moved to the validation set\n",
    "    num_val_files = int(len(files) * validation_percentage)\n",
    "\n",
    "    # Randomly select files for the validation set\n",
    "    val_files = random.sample(files, num_val_files)\n",
    "\n",
    "    # Move the selected files to the validation set directory\n",
    "    for file in val_files:\n",
    "        src_path = os.path.join(category_path, file)\n",
    "        dest_path = os.path.join(val_directory, category_folder, file)\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "        shutil.move(src_path, dest_path)\n",
    "\n",
    "# The remaining files in each category folder are part of the training set\n",
    "# You can optionally move them to the training set directory\n",
    "for category_folder in os.listdir(root_directory):\n",
    "    category_path = os.path.join(root_directory, category_folder)\n",
    "\n",
    "    # Get the list of files in the category folder\n",
    "    files = os.listdir(category_path)\n",
    "\n",
    "    # Move the remaining files to the training set directory\n",
    "    for file in files:\n",
    "        src_path = os.path.join(category_path, file)\n",
    "        dest_path = os.path.join(train_directory, category_folder, file)\n",
    "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "        shutil.move(src_path, dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d656c4f5-a974-4d66-a29e-72a367d3cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in automobile: 700\n",
      "Number of files in dog: 700\n",
      "Number of files in ship: 700\n",
      "Number of files in airplane: 700\n",
      "Number of files in cat: 700\n",
      "Number of files in deer: 700\n",
      "Number of files in truck: 700\n",
      "Number of files in frog: 700\n",
      "Number of files in horse: 700\n",
      "Number of files in bird: 701\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace \"path/to/your/data\" with the actual path to your data\n",
    "data_path = \"train\"\n",
    "\n",
    "# Loop through each category folder\n",
    "for category_folder in os.listdir(data_path):\n",
    "    category_path = os.path.join(data_path, category_folder)\n",
    "\n",
    "    # List all files in the category folder\n",
    "    files = os.listdir(category_path)\n",
    "\n",
    "    # Count the number of files in the category folder\n",
    "    num_files = len(files)\n",
    "\n",
    "    print(f\"Number of files in {category_folder}: {num_files}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
